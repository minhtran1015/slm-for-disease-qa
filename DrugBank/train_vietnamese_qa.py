#!/usr/bin/env python3
"""
Training script cho Dataset DrugBank Vi·ªát Nam M·ªü R·ªông
Bao g·ªìm c·∫£ thu·ªëc khoa h·ªçc v√† thu·ªëc Vi·ªát h√≥a ph·ªï bi·∫øn
"""

import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoTokenizer, AutoModelForSequenceClassification,
    TrainingArguments, Trainer, DataCollatorWithPadding
)
from datasets import Dataset as HFDataset
from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
import numpy as np

class VietnameseDrugDataset(Dataset):
    """PyTorch Dataset cho d·ªØ li·ªáu thu·ªëc Vi·ªát Nam."""
    
    def __init__(self, jsonl_file, tokenizer, max_length=512):
        self.data = []
        with open(jsonl_file, 'r', encoding='utf-8') as f:
            for line in f:
                self.data.append(json.loads(line))
        
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # K·∫øt h·ª£p instruction v√† input
        text = f"{item['instruction']} {item['input']}"
        
        # Tokenize
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        # Chuy·ªÉn ƒê√∫ng/Sai th√†nh binary labels
        label = 1 if item['output'] == 'ƒê√∫ng' else 0
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

def load_vietnamese_dataset(jsonl_file, tokenizer, max_length=512):
    """Load d·ªØ li·ªáu JSONL v√† chuy·ªÉn sang Hugging Face Dataset format."""
    data = []
    with open(jsonl_file, 'r', encoding='utf-8') as f:
        for line in f:
            data.append(json.loads(line))
    
    # Chu·∫©n b·ªã texts v√† labels
    texts = [f"{item['instruction']} {item['input']}" for item in data]
    labels = [1 if item['output'] == 'ƒê√∫ng' else 0 for item in data]
    
    # Tokenize t·∫•t c·∫£ texts
    encodings = tokenizer(
        texts,
        truncation=True,
        padding=True,
        max_length=max_length,
        return_tensors='pt'
    )
    
    # T·∫°o HF Dataset
    dataset_dict = {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'labels': labels
    }
    
    return HFDataset.from_dict(dataset_dict)

def compute_metrics(eval_pred):
    """T√≠nh metrics cho evaluation."""
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    
    accuracy = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average='weighted')
    precision, recall, _, _ = precision_recall_fscore_support(labels, predictions, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': recall
    }

def main():
    """H√†m training ch√≠nh."""
    
    # C·∫•u h√¨nh
    MODEL_NAME = "google/gemma-2b"  # ƒê·ªïi th√†nh "Qwen/Qwen2.5-0.5B" cho Qwen
    TRAIN_FILE = "drugbank_qa_vietnamese_extended_train.jsonl"
    TEST_FILE = "drugbank_qa_vietnamese_extended_test.jsonl"  
    OUTPUT_DIR = "./vietnamese-drugbank-model"
    MAX_LENGTH = 512
    
    print(f"üáªüá≥ B·∫Øt ƒë·∫ßu training v·ªõi model: {MODEL_NAME}")
    print(f"üìä S·ª≠ d·ª•ng dataset m·ªü r·ªông (bao g·ªìm thu·ªëc Vi·ªát Nam)")
    
    # Load tokenizer v√† model
    print("üì• ƒêang load tokenizer v√† model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    
    # Th√™m padding token n·∫øu ch∆∞a c√≥
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForSequenceClassification.from_pretrained(
        MODEL_NAME,
        num_labels=2,
        problem_type="single_label_classification"
    )
    
    # Chu·∫©n b·ªã datasets
    print("üìä ƒêang chu·∫©n b·ªã datasets...")
    train_dataset = load_vietnamese_dataset(TRAIN_FILE, tokenizer, MAX_LENGTH)
    test_dataset = load_vietnamese_dataset(TEST_FILE, tokenizer, MAX_LENGTH)
    
    print(f"   Training samples: {len(train_dataset):,}")
    print(f"   Test samples: {len(test_dataset):,}")
    
    # Ph√¢n t√≠ch dataset
    with open(TRAIN_FILE, 'r', encoding='utf-8') as f:
        train_data = [json.loads(line) for line in f]
    
    dung_count = sum(1 for item in train_data if item['output'] == 'ƒê√∫ng')
    sai_count = len(train_data) - dung_count
    
    print(f"   ƒê√∫ng: {dung_count:,} ({dung_count/len(train_data)*100:.1f}%)")
    print(f"   Sai: {sai_count:,} ({sai_count/len(train_data)*100:.1f}%)")
    
    # Data collator
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # Training arguments
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=3,
        per_device_train_batch_size=16,
        per_device_eval_batch_size=32,
        warmup_steps=500,
        weight_decay=0.01,
        logging_dir='./logs',
        logging_steps=100,
        evaluation_strategy="steps",
        eval_steps=500,
        save_strategy="steps",
        save_steps=1000,
        load_best_model_at_end=True,
        metric_for_best_model="f1",
        greater_is_better=True,
        report_to=None,  # T·∫Øt wandb/tensorboard logging
        save_total_limit=2,
        dataloader_pin_memory=False,
    )
    
    # Kh·ªüi t·∫°o trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=test_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )
    
    # Train model
    print("üèãÔ∏è B·∫Øt ƒë·∫ßu training...")
    trainer.train()
    
    # Evaluation cu·ªëi c√πng
    print("üìä Ch·∫°y evaluation cu·ªëi c√πng...")
    eval_results = trainer.evaluate()
    
    print(f"\n‚úÖ Training ho√†n th√†nh!")
    print(f"üìà K·∫øt qu·∫£ cu·ªëi c√πng:")
    for key, value in eval_results.items():
        if key.startswith('eval_'):
            metric_name = key.replace('eval_', '').title()
            print(f"   {metric_name}: {value:.4f}")
    
    # L∆∞u model
    print(f"üíæ ƒêang l∆∞u model v√†o: {OUTPUT_DIR}")
    trainer.save_model()
    tokenizer.save_pretrained(OUTPUT_DIR)
    
    print(f"\nüéâ Model s·∫µn s√†ng cho inference!")
    print(f"Load b·∫±ng: AutoModelForSequenceClassification.from_pretrained('{OUTPUT_DIR}')")

def vietnamese_inference_example():
    """V√≠ d·ª• c√°ch s·ª≠ d·ª•ng model ƒë√£ train cho inference v·ªõi thu·ªëc Vi·ªát Nam."""
    
    example_code = '''
# Load trained model cho thu·ªëc Vi·ªát Nam
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_path = "./vietnamese-drugbank-model"
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForSequenceClassification.from_pretrained(model_path)

# H√†m predict cho thu·ªëc Vi·ªát Nam
def predict_vietnamese_drug(instruction, question):
    text = f"{instruction} {question}"
    inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512)
    
    with torch.no_grad():
        outputs = model(**inputs)
        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
        predicted_class = torch.argmax(probabilities, dim=-1).item()
    
    return "ƒê√∫ng" if predicted_class == 1 else "Sai", probabilities[0][predicted_class].item()

# Test v·ªõi thu·ªëc Vi·ªát Nam
examples = [
    ("D·ª±a v√†o ki·∫øn th·ª©c v·ªÅ thu·ªëc, h√£y tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai.", 
     "Paracetamol c√≥ ph·∫£i l√† thu·ªëc gi·∫£m ƒëau kh√¥ng?"),
    ("X√°c ƒë·ªãnh xem ƒë√¢y c√≥ ph·∫£i l√† thu·ªëc. Tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai.", 
     "Thu·ªëc d·∫° d√†y Omeprazole c√≥ ph·∫£i l√† lo·∫°i virus kh√¥ng?"),
    ("S·ª≠ d·ª•ng hi·ªÉu bi·∫øt v·ªÅ d∆∞·ª£c ph·∫©m, tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai.",
     "Kh√°ng sinh Amoxicillin c√≥ ƒë∆∞·ª£c b√°n t·∫°i nh√† thu·ªëc kh√¥ng?"),
    ("ƒê√°nh gi√° xem ƒë√¢y c√≥ ph·∫£i thu·ªëc. Tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai.",
     "Vitamin C c√≥ ph·∫£i l√† m·ªôt b·ªánh truy·ªÅn nhi·ªÖm kh√¥ng?")
]

print("üáªüá≥ Test v·ªõi thu·ªëc Vi·ªát Nam:")
for instruction, question in examples:
    answer, confidence = predict_vietnamese_drug(instruction, question)
    print(f"Q: {question}")
    print(f"A: {answer} (ƒë·ªô tin c·∫≠y: {confidence:.3f})")
    print()
'''
    print("\nüîÆ V√≠ d·ª• Inference v·ªõi Thu·ªëc Vi·ªát Nam:")
    print("=" * 60)
    print(example_code)

if __name__ == "__main__":
    # Ch·∫°y training
    main()
    
    # Hi·ªÉn th·ªã v√≠ d·ª• inference
    vietnamese_inference_example()