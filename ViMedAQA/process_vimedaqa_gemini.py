#!/usr/bin/env python3
"""
Gemini API Pipeline for ViMedAQA Yes/No Statement Generation

This script processes ViMedAQA samples one by one using Gemini API to transform
Q&A pairs into True/False statement format for Vietnamese medical QA training.

Output format matches HPO bilingual dataset structure for consistent training.

Usage:
    1. Make sure .env file exists with GEMINI_API_KEY
    2. Run: python process_vimedaqa_gemini.py
    
This will process 10,000 random samples to generate ~20,000 balanced True/False statements.
"""

import os
import sys
import json
import time
import random
import pandas as pd
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

try:
    import google.generativeai as genai
except ImportError:
    print("‚ùå google-generativeai not installed. Run: pip install google-generativeai")
    sys.exit(1)

try:
    from dotenv import load_dotenv
    load_dotenv()  # Load environment variables from .env file
except ImportError:
    print("‚ö†Ô∏è python-dotenv not installed. Install with: pip install python-dotenv")
    print("   Falling back to manual environment variable loading...")

# ================= CONFIGURATION =================
SEED = 42
random.seed(SEED)

# API Configuration - OPTIMIZED FOR PAID API
GEMINI_MODEL = "gemini-2.5-flash"  # Updated to use Gemini 2.5 Flash
MAX_RETRIES = 1  # Single retry only
RETRY_DELAY = 0.1  # Minimal delay
REQUESTS_PER_MINUTE = 2000  # Aggressive rate for paid API

# File paths
INPUT_PARQUET = "train-00000-of-00001.parquet"
OUTPUT_FILE = "vimedaqa_yesno_gemini_10k_train.jsonl"
CHECKPOINT_FILE = "vimedaqa_gemini_checkpoint.json"
STATS_FILE = "vimedaqa_gemini_stats.json"

# Processing settings - OPTIMIZED FOR PAID API
SAVE_INTERVAL = 20  # Smaller batches for faster parallel processing
MAX_SAMPLES = 10000  # Hardcoded to process 10k samples (produces ~20k outputs)
BALANCE_RATIO = 1.0  # 1.0 = equal ƒê√∫ng/Sai samples

# Instructions for Gemini
INSTRUCTION_TEMPLATES = [
    "Tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai cho c√¢u h·ªèi y khoa sau.",
    "D·ª±a v√†o ki·∫øn th·ª©c y khoa, h√£y tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai.",
    "H√£y cho bi·∫øt c√¢u sau ƒë√∫ng hay sai d·ª±a v√†o ki·∫øn th·ª©c y khoa.",
    "X√°c ƒë·ªãnh t√≠nh ƒë√∫ng sai c·ªßa nh·∫≠n ƒë·ªãnh y khoa sau.",
    "V·ªõi ki·∫øn th·ª©c v·ªÅ y h·ªçc, h√£y x√°c nh·∫≠n c√¢u sau ƒê√∫ng hay Sai.",
]


def setup_gemini_api() -> genai.GenerativeModel:
    """Configure and return Gemini API client."""
    # Load API key from environment variables (.env file)
    api_key = os.getenv("GEMINI_API_KEY")
    
    if not api_key:
        raise ValueError(
            "GEMINI_API_KEY not found in environment variables.\n"
            "Make sure .env file exists with: GEMINI_API_KEY=your-api-key"
        )
    
    genai.configure(api_key=api_key)
    
    # Configure generation settings
    generation_config = genai.GenerationConfig(
        temperature=0.3,  # Lower temperature for consistent outputs
        max_output_tokens=500,
        top_p=0.9,
    )
    
    model = genai.GenerativeModel(
        model_name=GEMINI_MODEL,
        generation_config=generation_config,
    )
    
    return model


def create_statement_prompt(question: str, answer: str, context: str = "") -> str:
    """
    Create a prompt for Gemini to transform Q&A into a True/False statement.
    
    The goal is to generate statements like:
    - "Ho k√©o d√†i tr√™n 3 tu·∫ßn c√≥ ph·∫£i l√† tri·ªáu ch·ª©ng c·ªßa lao ph·ªïi."
    - "S·ªèi th·∫≠n h√¨nh th√†nh do kho√°ng ch·∫•t k·∫øt t·ª• trong n∆∞·ªõc ti·ªÉu."
    """
    prompt = f"""B·∫°n l√† chuy√™n gia y khoa Vi·ªát Nam. Nhi·ªám v·ª• c·ªßa b·∫°n l√† chuy·ªÉn ƒë·ªïi c·∫∑p c√¢u h·ªèi-tr·∫£ l·ªùi th√†nh m·ªôt c√¢u kh·∫≥ng ƒë·ªãnh y khoa ƒë·ªÉ ki·ªÉm tra ki·∫øn th·ª©c ƒê√∫ng/Sai.

**Quy t·∫Øc:**
1. T·∫°o M·ªòT c√¢u kh·∫≥ng ƒë·ªãnh ng·∫Øn g·ªçn, r√µ r√†ng b·∫±ng ti·∫øng Vi·ªát
2. C√¢u kh·∫≥ng ƒë·ªãnh ph·∫£i mang t√≠nh y khoa chuy√™n m√¥n
3. C√¢u ph·∫£i c√≥ th·ªÉ tr·∫£ l·ªùi ƒê√∫ng ho·∫∑c Sai m·ªôt c√°ch r√µ r√†ng
4. KH√îNG th√™m "(ƒê/S)" v√†o cu·ªëi c√¢u
5. Gi·ªØ nguy√™n thu·∫≠t ng·ªØ y khoa quan tr·ªçng
6. C√¢u ph·∫£i t·ª± nhi√™n v√† s√∫c t√≠ch

**V√≠ d·ª• ƒë·∫ßu v√†o:**
C√¢u h·ªèi: Paracetamol c√≥ ph·∫£i l√† m·ªôt lo·∫°i thu·ªëc gi·∫£m ƒëau kh√¥ng?
Tr·∫£ l·ªùi: C√≥, Paracetamol l√† thu·ªëc gi·∫£m ƒëau h·∫° s·ªët ph·ªï bi·∫øn.

**V√≠ d·ª• ƒë·∫ßu ra mong mu·ªën:**
Paracetamol l√† m·ªôt lo·∫°i thu·ªëc c√≥ t√°c d·ª•ng gi·∫£m ƒëau v√† h·∫° s·ªët.

---

**C√¢u h·ªèi:** {question}

**Tr·∫£ l·ªùi:** {answer}

{f"**Ng·ªØ c·∫£nh b·ªï sung:** {context}" if context else ""}

**C√¢u kh·∫≥ng ƒë·ªãnh y khoa (ch·ªâ tr·∫£ l·ªùi c√¢u kh·∫≥ng ƒë·ªãnh, kh√¥ng gi·∫£i th√≠ch):**"""
    
    return prompt


def create_false_statement_prompt(question: str, answer: str, context: str = "") -> str:
    """
    Create a prompt for Gemini to generate a FALSE medical statement.
    This creates a statement that sounds plausible but is medically incorrect.
    """
    prompt = f"""B·∫°n l√† chuy√™n gia y khoa Vi·ªát Nam. Nhi·ªám v·ª• c·ªßa b·∫°n l√† t·∫°o m·ªôt c√¢u kh·∫≥ng ƒë·ªãnh y khoa SAI (nh∆∞ng c√≥ v·∫ª h·ª£p l√Ω) d·ª±a tr√™n th√¥ng tin d∆∞·ªõi ƒë√¢y.

**Quy t·∫Øc:**
1. T·∫°o M·ªòT c√¢u kh·∫≥ng ƒë·ªãnh SAI v·ªÅ m·∫∑t y khoa
2. C√¢u ph·∫£i c√≥ v·∫ª h·ª£p l√Ω ƒë·ªÉ ki·ªÉm tra ki·∫øn th·ª©c y khoa
3. Thay ƒë·ªïi m·ªôt chi ti·∫øt quan tr·ªçng ƒë·ªÉ c√¢u tr·ªü th√†nh SAI (v√≠ d·ª•: thay ƒë·ªïi li·ªÅu l∆∞·ª£ng, c√¥ng d·ª•ng, c√°ch d√πng, t√°c d·ª•ng ph·ª•)
4. KH√îNG th√™m "(ƒê/S)" v√†o cu·ªëi c√¢u
5. C√¢u ph·∫£i t·ª± nhi√™n v√† s√∫c t√≠ch
6. SAI m·ªôt c√°ch tinh vi, kh√¥ng qu√° r√µ r√†ng

**V√≠ d·ª• ƒë·∫ßu v√†o:**
C√¢u h·ªèi: Paracetamol c√≥ ph·∫£i l√† m·ªôt lo·∫°i thu·ªëc gi·∫£m ƒëau kh√¥ng?
Tr·∫£ l·ªùi: C√≥, Paracetamol l√† thu·ªëc gi·∫£m ƒëau h·∫° s·ªët ph·ªï bi·∫øn.

**V√≠ d·ª• ƒë·∫ßu ra mong mu·ªën (c√¢u SAI):**
Paracetamol l√† thu·ªëc kh√°ng sinh ƒëi·ªÅu tr·ªã nhi·ªÖm khu·∫©n.

---

**C√¢u h·ªèi:** {question}

**Tr·∫£ l·ªùi:** {answer}

{f"**Ng·ªØ c·∫£nh b·ªï sung:** {context}" if context else ""}

**C√¢u kh·∫≥ng ƒë·ªãnh SAI v·ªÅ y khoa (ch·ªâ tr·∫£ l·ªùi c√¢u kh·∫≥ng ƒë·ªãnh, kh√¥ng gi·∫£i th√≠ch):**"""
    
    return prompt


def call_gemini_api(
    model: genai.GenerativeModel,
    prompt: str,
    retries: int = MAX_RETRIES
) -> Optional[str]:
    """
    Call Gemini API with minimal retry logic for paid API.
    
    Returns:
        Generated text or None if failed.
    """
    for attempt in range(retries + 1):
        try:
            response = model.generate_content(prompt)
            
            # Check if response has text
            if response and response.text:
                return response.text.strip()
            else:
                if attempt == 0:  # Only print on first failure
                    print(f"  ‚ö†Ô∏è Empty response")
                
        except Exception as e:
            error_str = str(e).lower()
            
            # For paid API, minimal waiting and quick failures
            if "rate" in error_str or "quota" in error_str or "429" in error_str:
                if attempt < retries:
                    time.sleep(0.1)  # Very short wait for paid API
                
            elif "api" in error_str or "500" in error_str or "404" in error_str:
                return None  # Immediate failure for API errors
                
            else:
                return None  # Immediate failure for other errors
    
    return None


def load_checkpoint() -> Dict[str, Any]:
    """Load processing checkpoint if exists."""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "processed_indices": [],
        "sampled_indices": None,  # Store the random sample indices for reproducibility
        "last_processed": -1,
        "total_samples": 0,
        "successful": 0,
        "failed": 0,
    }


def save_checkpoint(checkpoint: Dict[str, Any]):
    """Save processing checkpoint."""
    checkpoint["timestamp"] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, ensure_ascii=False, indent=2)


def save_samples(samples: List[Dict], filepath: str, mode: str = 'a'):
    """Save samples to JSONL file."""
    with open(filepath, mode, encoding='utf-8') as f:
        for sample in samples:
            json.dump(sample, f, ensure_ascii=False)
            f.write('\n')


def process_batch_samples(
    model: genai.GenerativeModel,
    batch_samples: List[pd.Series],
    batch_start_idx: int
) -> List[Dict[str, Any]]:
    """
    Process a batch of samples with parallel API calls.
    """
    results = []
    
    # Create all prompts first
    prompts = []
    sample_info = []
    
    for i, row in enumerate(batch_samples):
        question = str(row.get('question', '')).strip()
        answer = str(row.get('answer', '')).strip()
        context = str(row.get('context', '')).strip() if 'context' in row else ""
        
        if not question or not answer:
            continue
            
        # True statement prompt
        true_prompt = create_statement_prompt(question, answer, context)
        false_prompt = create_false_statement_prompt(question, answer, context)
        
        prompts.extend([true_prompt, false_prompt])
        sample_info.extend([
            (question, answer, True, batch_start_idx + i),
            (question, answer, False, batch_start_idx + i)
        ])
    
    # Process prompts with ThreadPoolExecutor for parallel API calls
    with ThreadPoolExecutor(max_workers=10) as executor:
        # Submit all API calls
        future_to_info = {}
        for i, prompt in enumerate(prompts):
            future = executor.submit(call_gemini_api, model, prompt)
            future_to_info[future] = sample_info[i]
            
            # Small delay between submissions for rate limiting
            time.sleep(60 / REQUESTS_PER_MINUTE / 10)  # Distributed delay
        
        # Collect results as they complete
        for future in as_completed(future_to_info):
            question, answer, is_true, sample_idx = future_to_info[future]
            
            try:
                statement = future.result()
                
                if statement:
                    # Clean the statement
                    statement = statement.replace("(ƒê/S)", "").strip().rstrip('.')
                    
                    result = {
                        "instruction": random.choice(INSTRUCTION_TEMPLATES),
                        "input": statement,
                        "output": "ƒê√∫ng" if is_true else "Sai",
                        "question_type": "vimedaqa_true" if is_true else "vimedaqa_false",
                        "source_question": question,
                        "source_answer": answer[:200] + "..." if len(answer) > 200 else answer,
                        "sample_idx": sample_idx
                    }
                    results.append(result)
                    
            except Exception as e:
                continue  # Skip failed samples
    
    return results


def main():
    """Main processing pipeline - Hardcoded to generate 20k balanced samples."""
    
    print("=" * 60)
    print("üöÄ ViMedAQA Gemini Processing Pipeline - 20k Output Generation")
    print("=" * 60)
    print(f"   Processing: {MAX_SAMPLES:,} samples ‚Üí ~{MAX_SAMPLES*2:,} outputs")
    print(f"   Model: {GEMINI_MODEL}")
    print(f"   Output: {OUTPUT_FILE}")
    
    # Setup
    print("\nüì° Setting up Gemini API...")
    try:
        model = setup_gemini_api()
        print(f"   ‚úÖ Connected to {GEMINI_MODEL}")
    except ValueError as e:
        print(f"   ‚ùå {e}")
        return
    
    # Load data
    print(f"\nüìÇ Loading data from {INPUT_PARQUET}...")
    try:
        df = pd.read_parquet(INPUT_PARQUET)
        total_samples = len(df)
        print(f"   ‚úÖ Loaded {total_samples:,} samples")
    except Exception as e:
        print(f"   ‚ùå Failed to load parquet: {e}")
        return
    
    # Load checkpoint
    checkpoint = load_checkpoint()
    processed_indices = set(checkpoint.get("processed_indices", []))
    
    if processed_indices:
        print(f"   üìç Resuming from checkpoint: {len(processed_indices)} already processed")
    
    # Random sampling for variety (but reproducible with seed)
    if MAX_SAMPLES < len(df):
        # Use consistent random sampling - save sampled indices to checkpoint for resume
        sampled_indices = checkpoint.get("sampled_indices", None)
        if sampled_indices is None:
            # First run - create random sample
            df_sampled = df.sample(n=MAX_SAMPLES, random_state=SEED)
            sampled_indices = df_sampled.index.tolist()
            checkpoint["sampled_indices"] = sampled_indices
            save_checkpoint(checkpoint)
            print(f"   üéØ Randomly sampled {MAX_SAMPLES:,} samples from {total_samples:,} total")
        else:
            # Resume - use saved indices
            df_sampled = df.loc[sampled_indices]
            print(f"   üìç Resumed with {MAX_SAMPLES:,} pre-sampled indices")
        
        df = df_sampled.reset_index(drop=True)
    else:
        df = df.head(MAX_SAMPLES)
        print(f"   üéØ Using {len(df):,} samples")
    
    # Initialize output file if new
    if not processed_indices and Path(OUTPUT_FILE).exists():
        # Backup existing file
        backup_name = f"{OUTPUT_FILE}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        Path(OUTPUT_FILE).rename(backup_name)
        print(f"   üì¶ Backed up existing file to {backup_name}")
    
    # Processing with batch optimization for paid API
    print(f"\nüîÑ Processing samples with batch optimization...")
    print(f"   Batch size: {SAVE_INTERVAL} samples")
    print(f"   Parallel workers: 10")
    print(f"   Rate limit: {REQUESTS_PER_MINUTE} requests/minute")
    print("-" * 60)
    
    batch_samples = []
    stats = {
        "total_processed": len(processed_indices),
        "successful_true": 0,
        "successful_false": 0,
        "failed": 0,
        "start_time": datetime.now().isoformat(),
    }
    
    try:
        # Convert processed_indices to work with reset dataframe indices
        original_processed = set(checkpoint.get("processed_indices", []))
        processed_df_indices = set()
        
        # Map original indices to new df indices for resume functionality
        if "sampled_indices" in checkpoint:
            for df_idx, orig_idx in enumerate(checkpoint["sampled_indices"]):
                if orig_idx in original_processed:
                    processed_df_indices.add(df_idx)
        
        with tqdm(total=len(df), desc="Processing batches", unit="sample", initial=len(processed_df_indices)) as pbar:
            
            batch = []
            batch_start_idx = 0
            
            for df_idx, row in df.iterrows():
                # Skip already processed
                if df_idx in processed_df_indices:
                    pbar.update(1)
                    continue
                
                batch.append(row)
                
                # Process batch when full or at end
                if len(batch) >= SAVE_INTERVAL or df_idx == len(df) - 1:
                    if batch:
                        # Process batch with parallel API calls
                        batch_results = process_batch_samples(model, batch, batch_start_idx)
                        
                        # Update tracking
                        for result in batch_results:
                            sample_idx = result.pop('sample_idx', 0)
                            processed_df_indices.add(sample_idx)
                            
                            # Map back to original index for checkpoint
                            if "sampled_indices" in checkpoint and checkpoint["sampled_indices"] is not None:
                                if sample_idx < len(checkpoint["sampled_indices"]):
                                    orig_idx = checkpoint["sampled_indices"][sample_idx]
                                    processed_indices.add(orig_idx)
                            else:
                                processed_indices.add(sample_idx)
                            
                            # Update stats
                            if result["output"] == "ƒê√∫ng":
                                stats["successful_true"] += 1
                            else:
                                stats["successful_false"] += 1
                        
                        batch_samples.extend(batch_results)
                        stats["total_processed"] = len(processed_indices)
                        
                        # Save batch
                        if batch_samples:
                            save_samples(batch_samples, OUTPUT_FILE, mode='a')
                            checkpoint["processed_indices"] = list(processed_indices)
                            checkpoint["last_processed"] = df_idx
                            checkpoint.update(stats)
                            save_checkpoint(checkpoint)
                            
                            tqdm.write(f"üíæ Batch saved: {len(batch_results)} statements from {len(batch)} samples")
                            batch_samples = []
                        
                        # Update progress
                        pbar.update(len(batch))
                        
                        batch = []
                        batch_start_idx = df_idx + 1
        
        # Save remaining
        if batch_samples:
            save_samples(batch_samples, OUTPUT_FILE, mode='a')
            checkpoint["processed_indices"] = list(processed_indices)
            checkpoint.update(stats)
            save_checkpoint(checkpoint)
            print(f"\n   üíæ Saved final {len(batch_samples)} samples")
    
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è Interrupted! Saving progress...")
        if batch_samples:
            save_samples(batch_samples, OUTPUT_FILE, mode='a')
        checkpoint["processed_indices"] = list(processed_indices)
        checkpoint.update(stats)
        save_checkpoint(checkpoint)
        print("   ‚úÖ Progress saved. Run again to resume.")
        return
    
    # Final statistics
    stats["end_time"] = datetime.now().isoformat()
    
    print("\n" + "=" * 60)
    print("üìä Processing Complete!")
    print("=" * 60)
    print(f"   Total samples processed: {stats['total_processed']}")
    print(f"   Successful TRUE statements: {stats['successful_true']}")
    print(f"   Successful FALSE statements: {stats['successful_false']}")
    print(f"   Failed: {stats['failed']}")
    print(f"   Output file: {OUTPUT_FILE}")
    
    # Save final stats
    with open(STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"   Stats saved to: {STATS_FILE}")
    
    # Show sample output
    if Path(OUTPUT_FILE).exists():
        print("\nüìù Sample outputs:")
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                sample = json.loads(line)
                print(f"\n   [{i+1}] {sample['input']}")
                print(f"       ‚Üí {sample['output']}")


if __name__ == "__main__":
    main()
