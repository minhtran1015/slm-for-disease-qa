#!/usr/bin/env python3
"""
Gemini API Pipeline for ViMedAQA Yes/No Statement Generation

This script processes ViMedAQA samples one by one using Gemini API to transform
Q&A pairs into True/False statement format for Vietnamese medical QA training.

Output format matches HPO bilingual dataset structure for consistent training.

Usage:
    1. Make sure .env file exists with GEMINI_API_KEY
    2. Run: python process_vimedaqa_gemini.py
    
This will process 10,000 random samples to generate ~20,000 balanced True/False statements.
"""

import os
import sys
import json
import time
import random
import pandas as pd
from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from typing import Optional, Dict, Any, List

try:
    import google.generativeai as genai
except ImportError:
    print("âŒ google-generativeai not installed. Run: pip install google-generativeai")
    sys.exit(1)

try:
    from dotenv import load_dotenv
    load_dotenv()  # Load environment variables from .env file
except ImportError:
    print("âš ï¸ python-dotenv not installed. Install with: pip install python-dotenv")
    print("   Falling back to manual environment variable loading...")

# ================= CONFIGURATION =================
SEED = 42
random.seed(SEED)

# API Configuration
GEMINI_MODEL = "gemini-2.5-flash"  # Updated to use Gemini 2.5 Flash
MAX_RETRIES = 3
RETRY_DELAY = 2  # seconds
REQUESTS_PER_MINUTE = 15  # Rate limiting

# File paths
INPUT_PARQUET = "train-00000-of-00001.parquet"
OUTPUT_FILE = "vimedaqa_yesno_gemini_10k_train.jsonl"
CHECKPOINT_FILE = "vimedaqa_gemini_checkpoint.json"
STATS_FILE = "vimedaqa_gemini_stats.json"

# Processing settings
SAVE_INTERVAL = 50  # Save checkpoint every N samples
MAX_SAMPLES = 10000  # Hardcoded to process 10k samples (produces ~20k outputs)
BALANCE_RATIO = 1.0  # 1.0 = equal ÄÃºng/Sai samples

# Instructions for Gemini
INSTRUCTION_TEMPLATES = [
    "Tráº£ lá»i ÄÃºng hoáº·c Sai cho cÃ¢u há»i y khoa sau.",
    "Dá»±a vÃ o kiáº¿n thá»©c y khoa, hÃ£y tráº£ lá»i ÄÃºng hoáº·c Sai.",
    "HÃ£y cho biáº¿t cÃ¢u sau Ä‘Ãºng hay sai dá»±a vÃ o kiáº¿n thá»©c y khoa.",
    "XÃ¡c Ä‘á»‹nh tÃ­nh Ä‘Ãºng sai cá»§a nháº­n Ä‘á»‹nh y khoa sau.",
    "Vá»›i kiáº¿n thá»©c vá» y há»c, hÃ£y xÃ¡c nháº­n cÃ¢u sau ÄÃºng hay Sai.",
]


def setup_gemini_api() -> genai.GenerativeModel:
    """Configure and return Gemini API client."""
    # Load API key from environment variables (.env file)
    api_key = os.getenv("GEMINI_API_KEY")
    
    if not api_key:
        raise ValueError(
            "GEMINI_API_KEY not found in environment variables.\n"
            "Make sure .env file exists with: GEMINI_API_KEY=your-api-key"
        )
    
    genai.configure(api_key=api_key)
    
    # Configure generation settings
    generation_config = genai.GenerationConfig(
        temperature=0.3,  # Lower temperature for consistent outputs
        max_output_tokens=500,
        top_p=0.9,
    )
    
    model = genai.GenerativeModel(
        model_name=GEMINI_MODEL,
        generation_config=generation_config,
    )
    
    return model


def create_statement_prompt(question: str, answer: str, context: str = "") -> str:
    """
    Create a prompt for Gemini to transform Q&A into a True/False statement.
    
    The goal is to generate statements like:
    - "Ho kÃ©o dÃ i trÃªn 3 tuáº§n cÃ³ pháº£i lÃ  triá»‡u chá»©ng cá»§a lao phá»•i."
    - "Sá»i tháº­n hÃ¬nh thÃ nh do khoÃ¡ng cháº¥t káº¿t tá»¥ trong nÆ°á»›c tiá»ƒu."
    """
    prompt = f"""Báº¡n lÃ  chuyÃªn gia y khoa Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  chuyá»ƒn Ä‘á»•i cáº·p cÃ¢u há»i-tráº£ lá»i thÃ nh má»™t cÃ¢u kháº³ng Ä‘á»‹nh y khoa Ä‘á»ƒ kiá»ƒm tra kiáº¿n thá»©c ÄÃºng/Sai.

**Quy táº¯c:**
1. Táº¡o Má»˜T cÃ¢u kháº³ng Ä‘á»‹nh ngáº¯n gá»n, rÃµ rÃ ng báº±ng tiáº¿ng Viá»‡t
2. CÃ¢u kháº³ng Ä‘á»‹nh pháº£i mang tÃ­nh y khoa chuyÃªn mÃ´n
3. CÃ¢u pháº£i cÃ³ thá»ƒ tráº£ lá»i ÄÃºng hoáº·c Sai má»™t cÃ¡ch rÃµ rÃ ng
4. KHÃ”NG thÃªm "(Ä/S)" vÃ o cuá»‘i cÃ¢u
5. Giá»¯ nguyÃªn thuáº­t ngá»¯ y khoa quan trá»ng
6. CÃ¢u pháº£i tá»± nhiÃªn vÃ  sÃºc tÃ­ch

**VÃ­ dá»¥ Ä‘áº§u vÃ o:**
CÃ¢u há»i: Paracetamol cÃ³ pháº£i lÃ  má»™t loáº¡i thuá»‘c giáº£m Ä‘au khÃ´ng?
Tráº£ lá»i: CÃ³, Paracetamol lÃ  thuá»‘c giáº£m Ä‘au háº¡ sá»‘t phá»• biáº¿n.

**VÃ­ dá»¥ Ä‘áº§u ra mong muá»‘n:**
Paracetamol lÃ  má»™t loáº¡i thuá»‘c cÃ³ tÃ¡c dá»¥ng giáº£m Ä‘au vÃ  háº¡ sá»‘t.

---

**CÃ¢u há»i:** {question}

**Tráº£ lá»i:** {answer}

{f"**Ngá»¯ cáº£nh bá»• sung:** {context}" if context else ""}

**CÃ¢u kháº³ng Ä‘á»‹nh y khoa (chá»‰ tráº£ lá»i cÃ¢u kháº³ng Ä‘á»‹nh, khÃ´ng giáº£i thÃ­ch):**"""
    
    return prompt


def create_false_statement_prompt(question: str, answer: str, context: str = "") -> str:
    """
    Create a prompt for Gemini to generate a FALSE medical statement.
    This creates a statement that sounds plausible but is medically incorrect.
    """
    prompt = f"""Báº¡n lÃ  chuyÃªn gia y khoa Viá»‡t Nam. Nhiá»‡m vá»¥ cá»§a báº¡n lÃ  táº¡o má»™t cÃ¢u kháº³ng Ä‘á»‹nh y khoa SAI (nhÆ°ng cÃ³ váº» há»£p lÃ½) dá»±a trÃªn thÃ´ng tin dÆ°á»›i Ä‘Ã¢y.

**Quy táº¯c:**
1. Táº¡o Má»˜T cÃ¢u kháº³ng Ä‘á»‹nh SAI vá» máº·t y khoa
2. CÃ¢u pháº£i cÃ³ váº» há»£p lÃ½ Ä‘á»ƒ kiá»ƒm tra kiáº¿n thá»©c y khoa
3. Thay Ä‘á»•i má»™t chi tiáº¿t quan trá»ng Ä‘á»ƒ cÃ¢u trá»Ÿ thÃ nh SAI (vÃ­ dá»¥: thay Ä‘á»•i liá»u lÆ°á»£ng, cÃ´ng dá»¥ng, cÃ¡ch dÃ¹ng, tÃ¡c dá»¥ng phá»¥)
4. KHÃ”NG thÃªm "(Ä/S)" vÃ o cuá»‘i cÃ¢u
5. CÃ¢u pháº£i tá»± nhiÃªn vÃ  sÃºc tÃ­ch
6. SAI má»™t cÃ¡ch tinh vi, khÃ´ng quÃ¡ rÃµ rÃ ng

**VÃ­ dá»¥ Ä‘áº§u vÃ o:**
CÃ¢u há»i: Paracetamol cÃ³ pháº£i lÃ  má»™t loáº¡i thuá»‘c giáº£m Ä‘au khÃ´ng?
Tráº£ lá»i: CÃ³, Paracetamol lÃ  thuá»‘c giáº£m Ä‘au háº¡ sá»‘t phá»• biáº¿n.

**VÃ­ dá»¥ Ä‘áº§u ra mong muá»‘n (cÃ¢u SAI):**
Paracetamol lÃ  thuá»‘c khÃ¡ng sinh Ä‘iá»u trá»‹ nhiá»…m khuáº©n.

---

**CÃ¢u há»i:** {question}

**Tráº£ lá»i:** {answer}

{f"**Ngá»¯ cáº£nh bá»• sung:** {context}" if context else ""}

**CÃ¢u kháº³ng Ä‘á»‹nh SAI vá» y khoa (chá»‰ tráº£ lá»i cÃ¢u kháº³ng Ä‘á»‹nh, khÃ´ng giáº£i thÃ­ch):**"""
    
    return prompt


def call_gemini_api(
    model: genai.GenerativeModel,
    prompt: str,
    retries: int = MAX_RETRIES
) -> Optional[str]:
    """
    Call Gemini API with retry logic and rate limiting.
    
    Returns:
        Generated text or None if failed.
    """
    for attempt in range(retries):
        try:
            response = model.generate_content(prompt)
            
            # Check if response has text
            if response and response.text:
                return response.text.strip()
            else:
                print(f"  âš ï¸ Empty response on attempt {attempt + 1}")
                
        except Exception as e:
            error_str = str(e).lower()
            
            # Handle rate limiting
            if "rate" in error_str or "quota" in error_str or "429" in error_str:
                wait_time = RETRY_DELAY * (attempt + 2)
                print(f"  â³ Rate limited. Waiting {wait_time}s...")
                time.sleep(wait_time)
                
            # Handle other API errors
            elif "api" in error_str or "500" in error_str:
                print(f"  âš ï¸ API error on attempt {attempt + 1}: {e}")
                time.sleep(RETRY_DELAY)
                
            else:
                print(f"  âŒ Unexpected error: {e}")
                return None
    
    return None


def load_checkpoint() -> Dict[str, Any]:
    """Load processing checkpoint if exists."""
    if Path(CHECKPOINT_FILE).exists():
        with open(CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    return {
        "processed_indices": [],
        "sampled_indices": None,  # Store the random sample indices for reproducibility
        "last_processed": -1,
        "total_samples": 0,
        "successful": 0,
        "failed": 0,
    }


def save_checkpoint(checkpoint: Dict[str, Any]):
    """Save processing checkpoint."""
    checkpoint["timestamp"] = datetime.now().isoformat()
    with open(CHECKPOINT_FILE, 'w', encoding='utf-8') as f:
        json.dump(checkpoint, f, ensure_ascii=False, indent=2)


def save_samples(samples: List[Dict], filepath: str, mode: str = 'a'):
    """Save samples to JSONL file."""
    with open(filepath, mode, encoding='utf-8') as f:
        for sample in samples:
            json.dump(sample, f, ensure_ascii=False)
            f.write('\n')


def process_single_sample(
    model: genai.GenerativeModel,
    row: pd.Series,
    generate_false: bool = True
) -> List[Dict[str, Any]]:
    """
    Process a single ViMedAQA sample and generate True/False statement pairs.
    
    Args:
        model: Gemini model instance
        row: Pandas row with question/answer/context
        generate_false: Whether to generate a false statement
    
    Returns:
        List of generated samples (1 or 2 depending on generate_false)
    """
    results = []
    
    question = str(row.get('question', '')).strip()
    answer = str(row.get('answer', '')).strip()
    context = str(row.get('context', '')).strip() if 'context' in row else ""
    
    if not question or not answer:
        return results
    
    # Rate limiting
    time.sleep(60 / REQUESTS_PER_MINUTE)
    
    # Generate TRUE statement
    true_prompt = create_statement_prompt(question, answer, context)
    true_statement = call_gemini_api(model, true_prompt)
    
    if true_statement:
        # Clean the statement
        true_statement = true_statement.replace("(Ä/S)", "").strip()
        true_statement = true_statement.rstrip('.')
        
        results.append({
            "instruction": random.choice(INSTRUCTION_TEMPLATES),
            "input": true_statement,
            "output": "ÄÃºng",
            "question_type": "vimedaqa_true",
            "source_question": question,
            "source_answer": answer[:200] + "..." if len(answer) > 200 else answer,
        })
    
    # Generate FALSE statement
    if generate_false:
        time.sleep(60 / REQUESTS_PER_MINUTE)  # Rate limiting
        
        false_prompt = create_false_statement_prompt(question, answer, context)
        false_statement = call_gemini_api(model, false_prompt)
        
        if false_statement:
            # Clean the statement
            false_statement = false_statement.replace("(Ä/S)", "").strip()
            false_statement = false_statement.rstrip('.')
            
            results.append({
                "instruction": random.choice(INSTRUCTION_TEMPLATES),
                "input": false_statement,
                "output": "Sai",
                "question_type": "vimedaqa_false",
                "source_question": question,
                "source_answer": answer[:200] + "..." if len(answer) > 200 else answer,
            })
    
    return results


def main():
    """Main processing pipeline - Hardcoded to generate 20k balanced samples."""
    
    print("=" * 60)
    print("ğŸš€ ViMedAQA Gemini Processing Pipeline - 20k Output Generation")
    print("=" * 60)
    print(f"   Processing: {MAX_SAMPLES:,} samples â†’ ~{MAX_SAMPLES*2:,} outputs")
    print(f"   Model: {GEMINI_MODEL}")
    print(f"   Output: {OUTPUT_FILE}")
    
    # Setup
    print("\nğŸ“¡ Setting up Gemini API...")
    try:
        model = setup_gemini_api()
        print(f"   âœ… Connected to {GEMINI_MODEL}")
    except ValueError as e:
        print(f"   âŒ {e}")
        return
    
    # Load data
    print(f"\nğŸ“‚ Loading data from {INPUT_PARQUET}...")
    try:
        df = pd.read_parquet(INPUT_PARQUET)
        total_samples = len(df)
        print(f"   âœ… Loaded {total_samples:,} samples")
    except Exception as e:
        print(f"   âŒ Failed to load parquet: {e}")
        return
    
    # Load checkpoint
    checkpoint = load_checkpoint()
    processed_indices = set(checkpoint.get("processed_indices", []))
    
    if processed_indices:
        print(f"   ğŸ“ Resuming from checkpoint: {len(processed_indices)} already processed")
    
    # Random sampling for variety (but reproducible with seed)
    if MAX_SAMPLES < len(df):
        # Use consistent random sampling - save sampled indices to checkpoint for resume
        sampled_indices = checkpoint.get("sampled_indices", None)
        if sampled_indices is None:
            # First run - create random sample
            df_sampled = df.sample(n=MAX_SAMPLES, random_state=SEED)
            sampled_indices = df_sampled.index.tolist()
            checkpoint["sampled_indices"] = sampled_indices
            save_checkpoint(checkpoint)
            print(f"   ğŸ¯ Randomly sampled {MAX_SAMPLES:,} samples from {total_samples:,} total")
        else:
            # Resume - use saved indices
            df_sampled = df.loc[sampled_indices]
            print(f"   ğŸ“ Resumed with {MAX_SAMPLES:,} pre-sampled indices")
        
        df = df_sampled.reset_index(drop=True)
    else:
        df = df.head(MAX_SAMPLES)
        print(f"   ğŸ¯ Using {len(df):,} samples")
    
    # Initialize output file if new
    if not processed_indices and Path(OUTPUT_FILE).exists():
        # Backup existing file
        backup_name = f"{OUTPUT_FILE}.backup.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        Path(OUTPUT_FILE).rename(backup_name)
        print(f"   ğŸ“¦ Backed up existing file to {backup_name}")
    
    # Processing
    print(f"\nğŸ”„ Processing samples...")
    print(f"   Save interval: every {SAVE_INTERVAL} samples")
    print(f"   Rate limit: {REQUESTS_PER_MINUTE} requests/minute")
    print(f"   Generate FALSE statements: True (for balance)")
    print("-" * 60)
    
    batch_samples = []
    stats = {
        "total_processed": len(processed_indices),
        "successful_true": 0,
        "successful_false": 0,
        "failed": 0,
        "start_time": datetime.now().isoformat(),
    }
    
    try:
        # Convert processed_indices to work with reset dataframe indices
        original_processed = set(checkpoint.get("processed_indices", []))
        processed_df_indices = set()
        
        # Map original indices to new df indices for resume functionality
        if "sampled_indices" in checkpoint:
            for df_idx, orig_idx in enumerate(checkpoint["sampled_indices"]):
                if orig_idx in original_processed:
                    processed_df_indices.add(df_idx)
        
        with tqdm(total=len(df), desc="Processing samples", unit="sample", initial=len(processed_df_indices)) as pbar:
            for df_idx, row in df.iterrows():
                # Skip already processed
                if df_idx in processed_df_indices:
                    pbar.update(1)
                    continue
            
                # Process sample (always generate both TRUE and FALSE)
                results = process_single_sample(model, row, generate_false=True)
                
                if results:
                    batch_samples.extend(results)
                    processed_df_indices.add(df_idx)
                    
                    # Map back to original index for checkpoint
                    if "sampled_indices" in checkpoint and checkpoint["sampled_indices"] is not None:
                        orig_idx = checkpoint["sampled_indices"][int(df_idx)]
                        processed_indices.add(orig_idx)
                    else:
                        processed_indices.add(df_idx)
                    
                    # Update stats
                    for r in results:
                        if r["output"] == "ÄÃºng":
                            stats["successful_true"] += 1
                        else:
                            stats["successful_false"] += 1
                    
                    stats["total_processed"] = len(processed_indices)
                else:
                    stats["failed"] += 1
                
                pbar.update(1)
            
                # Save periodically
                if len(batch_samples) >= SAVE_INTERVAL:
                    save_samples(batch_samples, OUTPUT_FILE, mode='a')
                    checkpoint["processed_indices"] = list(processed_indices)
                    checkpoint["last_processed"] = df_idx
                    checkpoint.update(stats)
                    save_checkpoint(checkpoint)
                    
                    tqdm.write(f"ğŸ’¾ Saved {len(batch_samples)} samples (Total: {stats['total_processed']})")
                    batch_samples = []
        
        # Save remaining
        if batch_samples:
            save_samples(batch_samples, OUTPUT_FILE, mode='a')
            checkpoint["processed_indices"] = list(processed_indices)
            checkpoint.update(stats)
            save_checkpoint(checkpoint)
            print(f"\n   ğŸ’¾ Saved final {len(batch_samples)} samples")
    
    except KeyboardInterrupt:
        print("\n\nâš ï¸ Interrupted! Saving progress...")
        if batch_samples:
            save_samples(batch_samples, OUTPUT_FILE, mode='a')
        checkpoint["processed_indices"] = list(processed_indices)
        checkpoint.update(stats)
        save_checkpoint(checkpoint)
        print("   âœ… Progress saved. Run again to resume.")
        return
    
    # Final statistics
    stats["end_time"] = datetime.now().isoformat()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Processing Complete!")
    print("=" * 60)
    print(f"   Total samples processed: {stats['total_processed']}")
    print(f"   Successful TRUE statements: {stats['successful_true']}")
    print(f"   Successful FALSE statements: {stats['successful_false']}")
    print(f"   Failed: {stats['failed']}")
    print(f"   Output file: {OUTPUT_FILE}")
    
    # Save final stats
    with open(STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    print(f"   Stats saved to: {STATS_FILE}")
    
    # Show sample output
    if Path(OUTPUT_FILE).exists():
        print("\nğŸ“ Sample outputs:")
        with open(OUTPUT_FILE, 'r', encoding='utf-8') as f:
            for i, line in enumerate(f):
                if i >= 3:
                    break
                sample = json.loads(line)
                print(f"\n   [{i+1}] {sample['input']}")
                print(f"       â†’ {sample['output']}")


if __name__ == "__main__":
    main()
